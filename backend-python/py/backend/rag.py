<<<<<<< HEAD
import os
=======
# ============================================================
# 1. ì„í¬íŠ¸ ë° ì„¤ì • êµ¬ì—­ (ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í™˜ê²½ ì„¤ì •)
# ============================================================
import os
import openai
>>>>>>> b0ab98293bedc6ec51b2aff874dc0d691bf6e534
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ì„¤ì •: DB ê²½ë¡œ ë° ì„ë² ë”© ëª¨ë¸ (ì ˆëŒ€ ê²½ë¡œë¡œ í†µì¼)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "chroma_db")
EMBEDDING_MODEL = "text-embedding-3-small"
LLM_MODEL = "gpt-4o"

<<<<<<< HEAD
=======

# ============================================================
# 2. í´ë˜ìŠ¤ ë° í•¨ìˆ˜ ì •ì˜ êµ¬ì—­ (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)
# ============================================================

>>>>>>> b0ab98293bedc6ec51b2aff874dc0d691bf6e534
class MockChatService:
    """
    API í‚¤ê°€ ì—†ì„ ë•Œ ë™ì‘í•˜ëŠ” ê°€ìƒì˜ ì±—ë´‡ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
    ì™¸ë¶€ ì—°ê²° ì—†ì´ ê³ ì •ëœ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    def __init__(self):
        print("[System] Running in MOCK Mode (No External Config)")
        
    def get_answer(self, message: str):
        return {
            "answer": f"[MockBot] ì•ˆë…•í•˜ì„¸ìš”! í˜„ì¬ í…ŒìŠ¤íŠ¸ ëª¨ë“œì…ë‹ˆë‹¤. ì…ë ¥í•˜ì‹  ë‚´ìš©ì€ '{message}' ì…ë‹ˆë‹¤. ì‹¤ì œ AI ì—°ê²°ì€ ë˜ì–´ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "context": [
                {"source": "mock_data.txt", "content": "This is a mock content needed for testing."}
            ]
        }

<<<<<<< HEAD
def initialize_rag_chain():
    """
    API Key Check -> Mock Mode or Real Mode
=======

def initialize_rag_chain():
    """
    API Key Check -> Mock Mode or Real Mode ì‹œìŠ¤í…œ ì´ˆê¸°í™”
>>>>>>> b0ab98293bedc6ec51b2aff874dc0d691bf6e534
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("\n" + "="*50)
        print("â›” OPENAI_API_KEY not found in .env")
        print("âœ… Switching to MOCK MODE (No external connection)")
        print("="*50 + "\n")
        return MockChatService()

    # --- Real RAG Initialization (Only if Key exists) ---
    print("ğŸ”‘ API Key found. Initializing Real RAG System...")
    
    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)
<<<<<<< HEAD
    llm = ChatOpenAI(model_name=LLM_MODEL, temperature=0) # <--- Moved Here (Global in function)
    
    # --- 1. Basic LLM Chain (Pure GPT) ---

    template = """
    ë‹¹ì‹ ì€ 'ë³´ì´ìŠ¤í”¼ì‹±/ìŠ¤ë¯¸ì‹± ì˜ˆë°© ì•ˆë‚´ AI'ì…ë‹ˆë‹¤.
    ì•„ë˜ì˜ [ê³µì‹ ê°€ì´ë“œë¼ì¸]ì„ ê·¼ê±°ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    
    [Context]
    {context}

    [User Question]
    {question}

    [Answer]
    """
    # --- 1. Basic LLM Chain (Pure GPT) ---
    # RAG ì—†ì´ ê·¸ëƒ¥ ëŒ€í™”í•˜ëŠ” ëª¨ë“œ (ë¹„êµ í…ŒìŠ¤íŠ¸ìš©)
=======
    llm = ChatOpenAI(model_name=LLM_MODEL, temperature=0)
    
    # 1. Basic LLM Chain (RAG ì—†ì´ ëŒ€í™”í•˜ëŠ” ëª¨ë“œ)
>>>>>>> b0ab98293bedc6ec51b2aff874dc0d691bf6e534
    basic_template = """
    ë‹¹ì‹ ì€ 'ë³´ì´ìŠ¤í”¼ì‹±/ìŠ¤ë¯¸ì‹± ì˜ˆë°© ì•ˆë‚´ AI'ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¹ì‹ ì´ ê°€ì§„ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
    ë‹¨, ë‹µë³€ ëì— "ì •í™•í•œ ì •ë³´ëŠ” ë°˜ë“œì‹œ ê³µì‹ ê°€ì´ë“œë¼ì¸ì„ í™•ì¸í•˜ì„¸ìš”."ë¼ê³  ë§ë¶™ì—¬ì£¼ì„¸ìš”.

    [User Question]
    {question}
    """
    basic_prompt = ChatPromptTemplate.from_template(basic_template)
    basic_chain = (
        {"question": RunnablePassthrough()}
        | basic_prompt
        | llm
        | StrOutputParser()
    )

<<<<<<< HEAD
    # --- 2. RAG Chain (With Chroma) ---
=======
    # 2. RAG Chain (Chroma DB ì—°ë™)
>>>>>>> b0ab98293bedc6ec51b2aff874dc0d691bf6e534
    rag_chain = None
    retriever = None

    if os.path.exists(DB_PATH):
        print("ğŸ“ DB found. Initializing RAG components...")
        vectorstore = Chroma(
            persist_directory=DB_PATH,
            embedding_function=embeddings,
            collection_name="phishing_guidelines"
        )
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )

        rag_template = """
        ë‹¹ì‹ ì€ 'ë³´ì´ìŠ¤í”¼ì‹±/ìŠ¤ë¯¸ì‹± ì˜ˆë°© ì•ˆë‚´ AI'ì…ë‹ˆë‹¤.
        ì•„ë˜ì˜ [ê³µì‹ ê°€ì´ë“œë¼ì¸]ì„ ê·¼ê±°ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
        
        [Context]
        {context}

        [User Question]
        {question}

        [Answer]
        ê·¼ê±°ê°€ ë¶€ì¡±í•˜ë©´ ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ê³  112/118 ì‹ ê³ ë¥¼ ì•ˆë‚´í•˜ì„¸ìš”.
        """
        rag_prompt = ChatPromptTemplate.from_template(rag_template)
        
        def format_docs(docs):
            return "\n\n".join(f"[ì¶œì²˜: {d.metadata.get('source', 'Unknown')}] {d.page_content}" for d in docs)

        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | rag_prompt
            | llm
            | StrOutputParser()
        )
    else:
        print("âš ï¸ No DB found. RAG mode will be disabled.")

    return {
        "basic_chain": basic_chain,
        "rag_chain": rag_chain,
        "retriever": retriever
    }

<<<<<<< HEAD
import openai # ì¶”ê°€

# ... (omitted)

def get_answer(rag_system, message: str, use_rag: bool = True):
    """
    RAG ì‘ë‹µ ìƒì„± (ì—ëŸ¬ í•¸ë“¤ë§ ì¶”ê°€)
=======

def get_answer(rag_system, message: str, use_rag: bool = True):
    """
    RAG ì‘ë‹µ ìƒì„± (ì—ëŸ¬ í•¸ë“¤ë§ í¬í•¨)
>>>>>>> b0ab98293bedc6ec51b2aff874dc0d691bf6e534
    """
    # 1. Mock Mode Check
    if isinstance(rag_system, MockChatService):
        return rag_system.get_answer(message)

    # 2. Decide Mode
    chain = rag_system.get("rag_chain")
    retriever = rag_system.get("retriever")

    if use_rag and not chain:
        print("[Info] RAG requested but DB not ready. Falling back to Basic LLM.")
        use_rag = False

    try:
        if use_rag and chain:
<<<<<<< HEAD
            # RAG Mode
=======
            # RAG Mode ì‹¤í–‰
>>>>>>> b0ab98293bedc6ec51b2aff874dc0d691bf6e534
            print(f"[Mode] RAG (Searching DB for: {message[:20]}...)")
            answer = chain.invoke(message)
            docs = retriever.invoke(message)
            sources = [
                {"content": d.page_content[:100] + "...", "source": d.metadata.get("source")} 
                for d in docs
            ]
            return {"answer": answer, "context": sources, "mode": "RAG"}
        else:
<<<<<<< HEAD
            # Pure LLM Mode
=======
            # Pure LLM Mode ì‹¤í–‰
>>>>>>> b0ab98293bedc6ec51b2aff874dc0d691bf6e534
            print("[Mode] Pure LLM (No DB Search)")
            answer = rag_system["basic_chain"].invoke(message)
            return {"answer": answer, "context": [], "mode": "Pure-LLM"}

    except openai.RateLimitError:
        print("âŒ OpenAI Quota Exceeded")
        return {
            "answer": "âš ï¸ ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ OpenAI API ì‚¬ìš© í•œë„(ì˜ˆì‚°)ê°€ ì´ˆê³¼ë˜ì–´ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Billing í™•ì¸ í•„ìš”)",
            "context": [],
            "mode": "Error-Quota"
        }
    except Exception as e:
        print(f"âŒ Error during generation: {e}")
        return {
            "answer": f"âš ï¸ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "context": [],
            "mode": "Error"
        }
<<<<<<< HEAD
=======


# ============================================================
# 3. í…ŒìŠ¤íŠ¸ ë° ì‹¤í–‰ êµ¬ì—­ (ì§ì ‘ ì‹¤í–‰ ì‹œì—ë§Œ)
# ============================================================

if __name__ == "__main__":
    # .env ë¡œë“œ (í…ŒìŠ¤íŠ¸ìš©)
    from dotenv import load_dotenv
    load_dotenv()
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = initialize_rag_chain()
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_q = "ëª¨ë¥´ëŠ” ë²ˆí˜¸ë¡œ íƒë°° ë¬¸ìê°€ ì™”ëŠ”ë° ë§í¬ë¥¼ ëˆŒëŸ¬ë„ ë¼?"
    print(f"\nQ: {test_q}")
    
    res = get_answer(rag_system, test_q)
    print(f"A: {res['answer']}")
    print("-" * 50)
>>>>>>> b0ab98293bedc6ec51b2aff874dc0d691bf6e534
