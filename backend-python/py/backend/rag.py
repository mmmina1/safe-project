import os
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ì„¤ì •: DB ê²½ë¡œ ë° ìž„ë² ë”© ëª¨ë¸ (ì ˆëŒ€ ê²½ë¡œë¡œ í†µì¼)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "chroma_db")
EMBEDDING_MODEL = "text-embedding-3-small"
LLM_MODEL = "gpt-4o"

class MockChatService:
    """
    API í‚¤ê°€ ì—†ì„ ë•Œ ë™ìž‘í•˜ëŠ” ê°€ìƒì˜ ì±—ë´‡ ì„œë¹„ìŠ¤ìž…ë‹ˆë‹¤.
    ì™¸ë¶€ ì—°ê²° ì—†ì´ ê³ ì •ëœ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    def __init__(self):
        print("[System] Running in MOCK Mode (No External Config)")
        
    def get_answer(self, message: str):
        return {
            "answer": f"[MockBot] ì•ˆë…•í•˜ì„¸ìš”! í˜„ìž¬ í…ŒìŠ¤íŠ¸ ëª¨ë“œìž…ë‹ˆë‹¤. ìž…ë ¥í•˜ì‹  ë‚´ìš©ì€ '{message}' ìž…ë‹ˆë‹¤. ì‹¤ì œ AI ì—°ê²°ì€ ë˜ì–´ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "context": [
                {"source": "mock_data.txt", "content": "This is a mock content needed for testing."}
            ]
        }

def initialize_rag_chain():
    """
    API Key Check -> Mock Mode or Real Mode
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("\n" + "="*50)
        print("â›” OPENAI_API_KEY not found in .env")
        print("âœ… Switching to MOCK MODE (No external connection)")
        print("="*50 + "\n")
        return MockChatService()

    # --- Real RAG Initialization (Only if Key exists) ---
    print("ðŸ”‘ API Key found. Initializing Real RAG System...")
    
    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)
    llm = ChatOpenAI(model_name=LLM_MODEL, temperature=0) # <--- Moved Here (Global in function)
    
    # --- 1. Basic LLM Chain (Pure GPT) ---

    template = """
    ë‹¹ì‹ ì€ 'ë³´ì´ìŠ¤í”¼ì‹±/ìŠ¤ë¯¸ì‹± ì˜ˆë°© ì•ˆë‚´ AI'ìž…ë‹ˆë‹¤.
    ì•„ëž˜ì˜ [ê³µì‹ ê°€ì´ë“œë¼ì¸]ì„ ê·¼ê±°ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    
    [Context]
    {context}

    [User Question]
    {question}

    [Answer]
    """
    # --- 1. Basic LLM Chain (Pure GPT) ---
    # RAG ì—†ì´ ê·¸ëƒ¥ ëŒ€í™”í•˜ëŠ” ëª¨ë“œ (ë¹„êµ í…ŒìŠ¤íŠ¸ìš©)
    basic_template = """
    ë‹¹ì‹ ì€ 'ë³´ì´ìŠ¤í”¼ì‹±/ìŠ¤ë¯¸ì‹± ì˜ˆë°© ì•ˆë‚´ AI'ìž…ë‹ˆë‹¤.
    ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¹ì‹ ì´ ê°€ì§„ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
    ë‹¨, ë‹µë³€ ëì— "ì •í™•í•œ ì •ë³´ëŠ” ë°˜ë“œì‹œ ê³µì‹ ê°€ì´ë“œë¼ì¸ì„ í™•ì¸í•˜ì„¸ìš”."ë¼ê³  ë§ë¶™ì—¬ì£¼ì„¸ìš”.

    [User Question]
    {question}
    """
    basic_prompt = ChatPromptTemplate.from_template(basic_template)
    basic_chain = (
        {"question": RunnablePassthrough()}
        | basic_prompt
        | llm
        | StrOutputParser()
    )

    # --- 2. RAG Chain (With Chroma) ---
    rag_chain = None
    retriever = None

    if os.path.exists(DB_PATH):
        print("ðŸ“ DB found. Initializing RAG components...")
        vectorstore = Chroma(
            persist_directory=DB_PATH,
            embedding_function=embeddings,
            collection_name="phishing_guidelines"
        )
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )

        rag_template = """
        ë‹¹ì‹ ì€ 'ë³´ì´ìŠ¤í”¼ì‹±/ìŠ¤ë¯¸ì‹± ì˜ˆë°© ì•ˆë‚´ AI'ìž…ë‹ˆë‹¤.
        ì•„ëž˜ì˜ [ê³µì‹ ê°€ì´ë“œë¼ì¸]ì„ ê·¼ê±°ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
        
        [Context]
        {context}

        [User Question]
        {question}

        [Answer]
        ê·¼ê±°ê°€ ë¶€ì¡±í•˜ë©´ ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ê³  112/118 ì‹ ê³ ë¥¼ ì•ˆë‚´í•˜ì„¸ìš”.
        """
        rag_prompt = ChatPromptTemplate.from_template(rag_template)
        
        def format_docs(docs):
            return "\n\n".join(f"[ì¶œì²˜: {d.metadata.get('source', 'Unknown')}] {d.page_content}" for d in docs)

        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | rag_prompt
            | llm
            | StrOutputParser()
        )
    else:
        print("âš ï¸ No DB found. RAG mode will be disabled.")

    return {
        "basic_chain": basic_chain,
        "rag_chain": rag_chain,
        "retriever": retriever
    }

import openai # ì¶”ê°€

# ... (omitted)

def get_answer(rag_system, message: str, use_rag: bool = True):
    """
    RAG ì‘ë‹µ ìƒì„± (ì—ëŸ¬ í•¸ë“¤ë§ ì¶”ê°€)
    """
    # 1. Mock Mode Check
    if isinstance(rag_system, MockChatService):
        return rag_system.get_answer(message)

    # 2. Decide Mode
    chain = rag_system.get("rag_chain")
    retriever = rag_system.get("retriever")

    if use_rag and not chain:
        print("[Info] RAG requested but DB not ready. Falling back to Basic LLM.")
        use_rag = False

    try:
        if use_rag and chain:
            # RAG Mode
            print(f"[Mode] RAG (Searching DB for: {message[:20]}...)")
            answer = chain.invoke(message)
            docs = retriever.invoke(message)
            sources = [
                {"content": d.page_content[:100] + "...", "source": d.metadata.get("source")} 
                for d in docs
            ]
            return {"answer": answer, "context": sources, "mode": "RAG"}
        else:
            # Pure LLM Mode
            print("[Mode] Pure LLM (No DB Search)")
            answer = rag_system["basic_chain"].invoke(message)
            return {"answer": answer, "context": [], "mode": "Pure-LLM"}

    except openai.RateLimitError:
        print("âŒ OpenAI Quota Exceeded")
        return {
            "answer": "âš ï¸ ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ìž¬ OpenAI API ì‚¬ìš© í•œë„(ì˜ˆì‚°)ê°€ ì´ˆê³¼ë˜ì–´ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Billing í™•ì¸ í•„ìš”)",
            "context": [],
            "mode": "Error-Quota"
        }
    except Exception as e:
        print(f"âŒ Error during generation: {e}")
        return {
            "answer": f"âš ï¸ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "context": [],
            "mode": "Error"
        }
